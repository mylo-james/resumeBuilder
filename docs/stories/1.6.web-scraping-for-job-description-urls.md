# Story 1.6: Web Scraping for Job Description URLs

## Status

Draft

## Story

**As a** user, **I want** to input a URL to a job description and have the application automatically scrape the content,
**so that** I don't have to manually copy and paste job descriptions.

## Acceptance Criteria

1. The backend can accept a URL parameter in addition to plain text.
2. Puppeteer is integrated to scrape content from job description URLs.
3. The scraped content is cleaned and formatted before being sent to the AI.
4. Error handling is implemented for invalid URLs or scraping failures.
5. The frontend includes a URL input field alongside the text area.

## Tasks / Subtasks

- [ ] Install and configure Puppeteer (AC: 2)
  - [ ] Install Puppeteer: `npm install puppeteer` [Source: architecture/architecture-tech-stack.md#tech-stack-overview]
  - [ ] Create web scraping service structure
- [ ] Create web scraping service (AC: 2, 3)
  - [ ] Implement `src/lib/services/scraper.ts` main scraping service [Source:
        architecture/architecture-project-structure.md#srclib]
  - [ ] Create `src/lib/utils/content-cleaner.ts` for content cleaning [Source:
        architecture/architecture-project-structure.md#srclib]
  - [ ] Create `src/lib/selectors/job-sites.ts` for CSS selectors [Source:
        architecture/architecture-project-structure.md#srclib]
  - [ ] Handle different website structures and content extraction
- [ ] Update API route to handle URLs (AC: 1)
  - [ ] Modify `/api/generate` to accept URL input
  - [ ] Add URL validation and processing
  - [ ] Integrate scraping with existing AI generation flow
- [ ] Update frontend with URL input (AC: 5)
  - [ ] Add URL input field to `InputForm.tsx`
  - [ ] Handle both text and URL input methods
  - [ ] Update UI to support both input types
- [ ] Implement error handling (AC: 4)
  - [ ] Handle invalid URLs, network timeouts, blocked scraping
  - [ ] Handle unsupported website formats and content extraction failures
  - [ ] Provide user-friendly error messages
- [ ] Write comprehensive tests
  - [ ] Unit tests for scraping service with mocked websites [Source:
        architecture/architecture-testing-strategy.md#testing-pyramid-overview]
  - [ ] Integration tests with mocked web content [Source:
        architecture/architecture-testing-strategy.md#testing-pyramid-overview]
  - [ ] E2E tests for URL input flow [Source: architecture/architecture-testing-strategy.md#testing-pyramid-overview]

## Dev Notes

- Previous Story Insights

  - Builds on Story 1.5's AI integration and API route structure.
  - Adds URL input capability to existing job description processing.

- Data Models

  - Request format: `{ jobDescription?: string, jobUrl?: string }`
  - Scraped content structure:
    `{ title: string, description: string, company: string, location?: string, requirements?: string[] }`
  - Error response format for scraping failures

- API Specifications

  - POST endpoint: `/api/generate` (updated from Story 1.5) [Source:
    architecture/architecture-project-structure.md#srcapp]
  - Accepts both text and URL input parameters
  - Integrates scraping with existing AI generation flow

- Component Specifications

  - Update `InputForm.tsx` to include URL input field
  - Maintain existing UI components and loading states
  - Add error handling for scraping failures

- File Locations

  - Scraping service: `src/lib/services/scraper.ts` [Source: architecture/architecture-project-structure.md#srclib]
  - Content cleaning: `src/lib/utils/content-cleaner.ts` [Source: architecture/architecture-project-structure.md#srclib]
  - Site selectors: `src/lib/selectors/job-sites.ts` [Source: architecture/architecture-project-structure.md#srclib]
  - Updated API route: `src/app/api/generate/route.ts` [Source: architecture/architecture-project-structure.md#srcapp]

- Testing Requirements

  - Mock web scraping responses for testing [Source:
    architecture/architecture-testing-strategy.md#testing-best-practices]
  - Test different website structures and error scenarios [Source:
    architecture/architecture-testing-strategy.md#testing-pyramid-overview]
  - Verify content cleaning and formatting [Source:
    architecture/architecture-testing-strategy.md#testing-pyramid-overview]

- Technical Constraints
  - Handle Puppeteer browser management and cleanup
  - Implement timeouts for web scraping operations
  - Support multiple job site formats (LinkedIn, Indeed, Glassdoor, generic)

### Testing

- Test file location: Unit tests near services, integration tests under `tests/integration/`, E2E tests under
  `tests/e2e/` [Source: architecture/architecture-testing-strategy.md#test-file-organization]
- Test standards: Mock web scraping, test error scenarios, verify content cleaning [Source:
  architecture/architecture-testing-strategy.md#testing-best-practices]
- Frameworks/patterns: Jest for unit/integration with mocked websites, Playwright for E2E [Source:
  architecture/architecture-tech-stack.md#tech-stack-overview]
- Story-specific: Test URL input, web scraping, content cleaning, error handling for various failure scenarios

## Change Log

| Date | Version | Description | Author |
| ---- | ------- | ----------- | ------ |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## Critical Dev Flow

1. **Pull latest main**: `git pull origin main`
2. **Create story branch**: `git checkout -b feature/story-1.6-web-scraping-for-job-description-urls`
3. **Follow Red-Green-Refactor cycle**:
   - **Red**: Write failing tests first
   - **Green**: Implement minimal code to make tests pass
   - **Refactor**: Clean up code while keeping tests green
4. **Fulfill requirements**: Complete all acceptance criteria
5. **Verify quality**:
   - Run tests: `npm test && npm run test:e2e`
   - Run linting: `npm run lint`
   - Run type checking: `npm run type-check`
6. **Commit and push**: `git push origin feature/story-1.6-web-scraping-for-job-description-urls`
7. **Create PR**: Submit pull request for review
8. **Monitor CI**: Use `gh pr checks` to watch CI status and ensure all checks pass

## QA Results
